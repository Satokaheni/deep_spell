{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AlbertTokenizer, AlbertModel, AlbertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def gelu(x):\n",
    "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def gelu_new(x):\n",
    "    \"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def mish(x):\n",
    "    return x * torch.tanh(nn.functional.softplus(x))\n",
    "\n",
    "\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish, \"gelu_new\": gelu_new, \"mish\": mish}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder for Model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.embedding_size)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n",
    "        self.decoder = nn.Linear(config.embedding_size, config.vocab_size)\n",
    "        self.activation = ACT2FN[config.hidden_act]\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "\n",
    "        prediction_scores = hidden_states + self.bias\n",
    "\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpellCheck Model\n",
    "class AlbertSpellCheck(AlbertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.albert = AlbertModel.from_pretrained('../data/albert')\n",
    "        self.predictions = Decoder(config)\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.tie_weights()\n",
    "        \n",
    "    def tie_weights(self):\n",
    "        self._tie_or_clone_weights(self.predictions.decoder, self.albert.embeddings.word_embeddings)\n",
    "        \n",
    "    def get_output_embeddings(self):\n",
    "        return self.predictions.decoder\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None, \n",
    "        attention_mask=None, \n",
    "        token_type_ids=None, \n",
    "        position_ids=None, \n",
    "        head_mask=None, \n",
    "        input_embeds=None, \n",
    "        masked_lm_labels=None\n",
    "    ):\n",
    "        outputs = self.albert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            input_embeds=input_embeds\n",
    "        )\n",
    "        \n",
    "        sequence_outputs = outputs[0]\n",
    "        \n",
    "        prediction_scores = self.predictions(sequence_outputs)\n",
    "        \n",
    "        outputs = (prediction_scores,) + outputs[2:]\n",
    "        if masked_lm_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
    "            outputs = (masked_lm_loss,) + outputs\n",
    "            \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "with open('../data/bookcorpus/preprocess_p1.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to implement spelling errors\n",
    "def add_spelling_errors(token, error_rate=.4):\n",
    "    CHARS = list('abcdefghijklmnopqrstuvwxyz ')\n",
    "    \n",
    "    \"\"\"Simulate some artificial spelling mistakes.\"\"\"\n",
    "    assert(0.0 <= error_rate < 1.0)\n",
    "    if len(token) < 3:\n",
    "        return token\n",
    "    rand = np.random.rand()\n",
    "    # Here are 4 different ways spelling mistakes can occur,\n",
    "    # each of which has equal chance.\n",
    "    prob = error_rate / 4.0\n",
    "    if rand < prob:\n",
    "        # Replace a character with a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
    "                + token[random_char_index + 1:]\n",
    "    elif prob < rand < prob * 2:\n",
    "        # Delete a character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + token[random_char_index + 1:]\n",
    "    elif prob * 2 < rand < prob * 3:\n",
    "        # Add a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
    "                + token[random_char_index:]\n",
    "    elif prob * 3 < rand < prob * 4:\n",
    "        # Transpose 2 characters.\n",
    "        random_char_index = np.random.randint(len(token) - 1)\n",
    "        token = token[:random_char_index]  + token[random_char_index + 1] \\\n",
    "                + token[random_char_index] + token[random_char_index + 2:]\n",
    "    else:\n",
    "        # No spelling errors.\n",
    "        pass\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce spelling errors into the dataset\n",
    "spelling_errors = [' '.join([add_spelling_errors(w) for w in sent.split()]) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
